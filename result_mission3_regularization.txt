Partie logitic:			

	A:without regularization
		changing num_steps:
		1:train_subset = 10000  num_steps = 801 
			Loss at step 800: 1.061043
			Training accuracy: 79.3%
			Validation accuracy: 76.5%
			Test accuracy: 82.9%

		2: train_subset = 10000  num_steps = 1001
			Loss at step 1000: 0.988524
			Training accuracy: 80.1%
			Validation accuracy: 76.5%
			Test accuracy: 82.7%

		3: train_subset = 10000  num_steps = 1501
			Loss at step 1500: 0.794954
			Training accuracy: 81.7%
			Validation accuracy: 76.5%
			Test accuracy: 83.6%

		4: train_subset = 10000  num_steps = 2001
			Loss at step 2000: 0.699325
			Training accuracy: 83.4%
			Validation accuracy: 77.0%
			Test accuracy: 83.6%

		5: train_subset = 10000  num_steps = 3001
			Loss at step 3000: 0.583642
			Training accuracy: 85.1%
			Validation accuracy: 77.2%
			Test accuracy: 84.0%

		6: train_subset = 10000  num_steps = 5001
			Loss at step 5000: 0.464177
			Training accuracy: 87.5%
			Validation accuracy: 77.4%
			Test accuracy: 84.6%

		7: train_subset = 10000  num_steps = 10001
			Loss at step 10000: 0.357540
			Training accuracy: 90.6%
			Validation accuracy: 77.1%
			Test accuracy: 84.4%

		8: train_subset = 10000  num_steps = 12001
			Loss at step 12000: 0.335088
			Training accuracy: 91.5%
			Validation accuracy: 77.4%
			Test accuracy: 84.5%


		changing train_subset:
		1: train_subset = 10000  num_steps = 3001(由于初始化的原因每次得到的结果可能有一定的误差)
			Loss at step 3000: 0.590515
			Training accuracy: 85.0%
			Validation accuracy: 77.2%
			Test accuracy: 84.4%

		2: train_subset = 50000  num_steps = 3001
			Loss at step 3000: 0.819328
			Training accuracy: 80.3%
			Validation accuracy: 79.1%
			Test accuracy: 85.8%

		3：train_subset = 100000  num_steps = 3001
			Loss at step 3000: 0.849752
			Training accuracy: 80.0%
			Validation accuracy: 79.5%
			Test accuracy: 86.3%

	B:with regularization
		regularization系数为1：
		1: train_subset = 10000  num_steps = 3001
			Loss at step 3000: 1.925143
			Training accuracy: 56.5%
			Validation accuracy: 56.8%
			Test accuracy: 61.2%


		regularization系数为0.1：
			1:train_subset = 10000  num_steps = 30001
				Loss at step 3000: 0.969682
				Training accuracy: 82.6%
				Validation accuracy: 81.1%
				Test accuracy: 87.3%

			2:train_subset = 10000  num_steps = 12001
				Loss at step 12000: 0.969682
				Training accuracy: 82.6%
				Validation accuracy: 81.1%
				Test accuracy: 87.3%

		regularization系数为0.01：
			1:train_subset = 10000  num_steps = 12001
				Loss at step 12000: 0.661696
				Training accuracy: 85.1%
				Validation accuracy: 82.7%
				Test accuracy: 88.6%

		regularization系数为0.001：
			1:train_subset = 10000  num_steps = 12001
				Loss at step 12000: 0.507894
				Training accuracy: 88.0%
				Validation accuracy: 81.5%
				Test accuracy: 88.1%
		
		regularization系数为0.0001：		
			1:train_subset = 10000  num_steps = 12001
				Loss at step 12000: 0.414600
				Training accuracy: 91.4%
				Validation accuracy: 78.9%
				Test accuracy: 85.9%

	conclusion:regularization 可以很好的防止overfitting。但是系数要取的恰到好处。 过大后会使数据无法拟合如1，数据过小后无法起到防止overfitting的现象如0.0001。但是对
	神经网络的情况下最好还是重新设定实验数据来验证，以防与简单的逻辑回归有较大的区别。



Partie neural network with 1 hidden layer:
	A:without regularization 总共的数据个数为200000
		1.batch_size = 128 num_steps = 3001(数据轮流两遍左右，代表每个数据基本上都两次经过神经网络)
			Minibatch loss at step 3000 : 3.59768
			Minibatch accuracy: 81.2%
			Validation accuracy: 82.5%
			Test accuracy: 88.7%

		2.batch_size = 18 num_steps = 3001
			Minibatch accuracy: 83.3%
			Validation accuracy: 67.8%
			Test accuracy: 74.1%

		3.batch_size = 18 num_steps = 5001
			Minibatch loss at step 5000 : 3.50587
			Minibatch accuracy: 66.7%
			Validation accuracy: 73.1%
			Test accuracy: 79.1%

		4.batch_size = 18 num_steps = 10001
			Minibatch loss at step 10000 : 1.59011
			Minibatch accuracy: 55.6%
			Validation accuracy: 73.2%
			Test accuracy: 80.6%

		5.batch_size = 128 num_steps = 6001(数据轮流四遍左右，代表每个数据基本上都两次经过神经网络6001*128/200000=3.8)
			Minibatch loss at step 6000 : 1.15078
			Minibatch accuracy: 89.8%
			Validation accuracy: 84.0%
			Test accuracy: 90.0%

		6.batch_size = 128 num_steps = 12001
			Minibatch loss at step 12000 : 0.593247
			Minibatch accuracy: 90.6%
			Validation accuracy: 85.3%
			Test accuracy: 91.5%

		7.batch_size = 128 num_steps = 20001
			Minibatch loss at step 20000 : 0.474834
			Minibatch accuracy: 96.1%
			Validation accuracy: 84.5%
			Test accuracy: 90.2%

		8.batch_size = 128 num_steps = 30001
			Minibatch loss at step 30000 : 0.18618
			Minibatch accuracy: 95.3%
			Validation accuracy: 86.7%
			Test accuracy: 92.5%

		9.batch_size = 128 num_steps = 50001
			Minibatch loss at step 50000 : 0.0767919
			Minibatch accuracy: 98.4%(有的甚至会达到100)
			Validation accuracy: 87.2%
			Test accuracy: 93.2%

		9.batch_size = 128 num_steps = 80001
			Minibatch loss at step 80000 : 0.0439926
			Minibatch accuracy: 98.4%
			Validation accuracy: 87.6%
			Test accuracy: 93.3%

	B:without regularization 总共的数据个数为10000
		1:batch_size = 128 train_subset = 10000 num_steps = 3001
			Minibatch loss at step 3000 : 0.134275
			Minibatch accuracy: 98.4%
			Validation accuracy: 82.8%
			Test accuracy: 89.4%

		2:batch_size = 128 train_subset = 10000 num_steps = 12001
			Minibatch loss at step 12000 : 0.539284
			Minibatch accuracy: 99.2%
			Validation accuracy: 82.5%
			Test accuracy: 89.1%

		3:batch_size = 128 train_subset = 10000 num_steps = 20001
			Minibatch loss at step 20000 : 1.49409e-05
			Minibatch accuracy: 100.0%
			Validation accuracy: 82.7%
			Test accuracy: 89.6%

		3:batch_size = 128 train_subset = 10000 num_steps = 50001
			Minibatch loss at step 50000 : 0.000101309
			Minibatch accuracy: 100.0%
			Validation accuracy: 82.0%
			Test accuracy: 88.5%

		4:batch_size = 128 train_subset = 5000 num_steps = 50001
			Minibatch loss at step 50000 : 0.000403245
			Minibatch accuracy: 100.0%
			Validation accuracy: 81.4%
			Test accuracy: 87.8%

		5:batch_size = 128 train_subset = 3000 num_steps = 50001（overfitting exemple）
			Minibatch loss at step 50000 : 0.000590636
			Minibatch accuracy: 100.0%（几乎对每个nimibatch都完全吻合。不过由此也可以看出神经网络比单独的逻辑回归要表现好很多。hidden 	layer主要的作用就是给神经网络变成非线性，给它更多的可能性。）
			Validation accuracy: 80.0%
			Test accuracy: 86.9%

	C:with regularization 总共的数据个数为200000
		1:batch_size = 128 train_subset = 20000 num_steps = 120001, delta=0.01(regularization)
			Minibatch loss at step 120000 : 0.638186
			Minibatch accuracy: 88.3%
			Validation accuracy: 84.3%
			Test accuracy: 90.1%

		2:batch_size = 128 train_subset = 200000 num_steps = 120001, delta=0.0(regularization)
			Minibatch loss at step 120000 : 0.022771
			Minibatch accuracy: 99.2%
			Validation accuracy: 87.6%
			Test accuracy: 93.4%

		3:batch_size = 128 train_subset = 200000 num_steps = 120001, delta=0.001(regularization)   delta减小后test accuracy变大。再对比minibatch accuracy 可以发现比较好的避免了overfitting
			Minibatch loss at step 120000 : 0.40102
			Minibatch accuracy: 90.6%
			Validation accuracy: 89.4%
			Test accuracy: 94.4%

		4:batch_size = 128 train_subset = 200000 num_steps = 120001, delta=0.0005(regularization)
			Minibatch loss at step 120000 : 0.342716
			Minibatch accuracy: 92.2%
			Validation accuracy: 90.5%
			Test accuracy: 95.1%

		5:batch_size = 128 train_subset = 200000 num_steps = 50001, delta=0.0005(regularization) 方便与上面没有regularization的进行比较
			Minibatch loss at step 50000 : 0.451734
			Minibatch accuracy: 93.0%
			Validation accuracy: 89.6%
			Test accuracy: 94.8%

		6:batch_size = 128 train_subset = 200000 num_steps = 50001, delta=0.0001(regularization) 方便与上面没有regularization的进行比较
			Minibatch loss at step 50000 : 0.454266
			Minibatch accuracy: 97.7%
			Validation accuracy: 90.5%
			Test accuracy: 95.4%

		7:batch_size = 128 train_subset = 200000 num_steps = 80001, delta=0.0001(regularization) 方便与上面没有regularization的进行比较
			Minibatch loss at step 80000 : 0.235733
			Minibatch accuracy: 97.7%
			Validation accuracy: 90.5%
			Test accuracy: 95.6%

		8:batch_size = 128 train_subset = 200000 num_steps = 80001, delta=0.0005(regularization) 
			Minibatch loss at step 80000 : 0.332178
			Minibatch accuracy: 94.5%
			Validation accuracy: 90.2%
			Test accuracy: 95.3%

	D:with regularization, with dropout 总共的数据个数为200000   loss = loss + 0.0005*tf.nn.l2_loss(weights1)+ 0.0005*tf.nn.l2_loss(weights2)
		1:batch_size = 128 train_subset = 200000 num_steps = 80001, delta=0(without regularization)
			Minibatch loss at step 80000 : 0.275506
			Minibatch accuracy: 91.4%
			Validation accuracy: 87.5%
			Test accuracy: 93.2%

		2:batch_size = 128 train_subset = 200000 num_steps = 120001, delta=0(without regularization)
			Minibatch loss at step 120000 : 0.290193
			Minibatch accuracy: 89.8%
			Validation accuracy: 87.8%
			Test accuracy: 93.5%

		3:batch_size = 128 train_subset = 3000 num_steps = 50001, delta=0(without regularization)
			What happens to our extreme overfitting case?
				with dropout:
					Minibatch loss at step 50000 : 0.0284062
					Minibatch accuracy: 99.2%
					Validation accuracy: 82.2%
					Test accuracy: 88.6% 														

				without dropout:
					Minibatch loss at step 50000 : 0.000590636
					Minibatch accuracy: 100.0%（几乎对每个nimibatch都完全吻合。不过由此也可以看出神经网络比单独的逻辑回归要表现好很多。hidden 	layer主要的作用就是给神经网络变成非线性，给它更多的可能性。）
					Validation accuracy: 80.0%
					Test accuracy: 86.9%


Partie neural network with 2 hidden layer(1024,512):

	 A:without dropout hidden_layer_1_stddev = np.sqrt(2.0/784)  hidden_layer_2_stddev = np.sqrt(2.0/1024) output_layer_stddev = np.sqrt(2.0/512)
		1:batch_size = 128 train_subset = 200000 num_steps = 30001 (without regularization)
			Minibatch loss at step 30000 : 0.0250021
			Minibatch accuracy: 98.4%	(overfitting)
			Validation accuracy: 90.7%
			Test accuracy: 95.8%
		2: batch_size = 128 train_subset = 200000  num_steps = 30001 with regularization 0.001, without dropout
			Minibatch loss at step 30000 : 0.459998
			Minibatch accuracy: 90.6%
			Validation accuracy: 89.2%
			Test accuracy: 94.1%

		3: batch_size = 128 train_subset = 200000  num_steps = 30001 with regularization 0.0005
			Minibatch loss at step 30000 : 0.394141
			Minibatch accuracy: 93.8%
			Validation accuracy: 90.4%
			Test accuracy: 95.4%

		4: batch_size = 128 train_subset = 200000  num_steps = 30001 with regularization 0.0001
			Minibatch loss at step 30000 : 0.184428
			Minibatch accuracy: 98.4%
			Validation accuracy: 90.1%
			Test accuracy: 95.4%

		5: batch_size = 128 train_subset = 200000  num_steps = 80001 with regularization 0.0005(感觉取0.005时效果最好)
			Minibatch loss at step 80000 : 0.347538
			Minibatch accuracy: 95.3%
			Validation accuracy: 90.5%
			Test accuracy: 95.4%

	 B: only with dropout 0.5
	 loss = loss + (tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2)+tf.nn.l2_loss(weights3))*0.0005(去掉)
	 	1:batch_size = 128 train_subset = 200000 num_steps = 30001
	 		Minibatch loss at step 30000 : 0.310795
			Minibatch accuracy: 90.6%（比较好的解决了overfitting但是反而精确度会下降）
			Validation accuracy: 90.5%
			Test accuracy: 95.4%

		2：batch_size = 128 train_subset = 200000 num_steps = 80001
			Minibatch loss at step 80000 : 0.176295
			Minibatch accuracy: 93.0%
			Validation accuracy: 91.1%
			Test accuracy: 95.8%（dropout 在训练次数很大时也会很好的解决overfitting效果很regularization类似）



	C: with dropout 0.5， with regularization 0.0005
		batch_size = 128 train_subset = 200000 num_steps = 80001
		Minibatch loss at step 80000 : 0.399335
		Minibatch accuracy: 92.2%
		Validation accuracy: 89.9%
		Test accuracy: 94.9%

Partie neural network with 3 hidden layer(1024,512,256):

	1: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0 regularization=0
		Minibatch loss at step 80000 : 0.0265858
		Minibatch accuracy: 99.2%
		Validation accuracy: 90.9%
		Test accuracy: 95.9%

	2: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0 regularization=0.0005
		Minibatch loss at step 80000 : 0.346357
		Minibatch accuracy: 96.9%
		Validation accuracy: 90.0%
		Test accuracy: 94.9%

	3: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0 regularization=0.0001
		Minibatch loss at step 80000 : 0.215369
		Minibatch accuracy: 97.7%
		Validation accuracy: 91.3%
		Test accuracy: 95.9%

	4: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0 regularization=0.0003(随着训练次数的增加提升比较小，测试好模型后再test的数量变大再试)
		Minibatch loss at step 20000 : 0.327075
		Minibatch accuracy: 96.1%
		Validation accuracy: 90.3%
		Test accuracy: 95.3%
		Minibatch loss at step 30000 : 0.491729
		Minibatch accuracy: 93.0%
		Validation accuracy: 90.0%
		Test accuracy: 95.0%
		Minibatch loss at step 40000 : 0.35298
		Minibatch accuracy: 94.5%
		Validation accuracy: 89.7%
		Test accuracy: 95.2%
		Minibatch loss at step 50000 : 0.326905
		Minibatch accuracy: 96.1%
		Validation accuracy: 88.7%
		Test accuracy: 93.5%
		Minibatch loss at step 60000 : 0.358421
		Minibatch accuracy: 93.8%
		Validation accuracy: 90.1%
		Test accuracy: 95.0%
		Minibatch loss at step 70000 : 0.408444
		Minibatch accuracy: 92.2%
		Validation accuracy: 90.4%
		Test accuracy: 95.0%
		Minibatch loss at step 80000 : 0.333159
		Minibatch accuracy: 96.9%
		Validation accuracy: 90.6%
		Test accuracy: 95.6%

	5: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0 regularization=0.0002
		Minibatch loss at step 80000 : 0.269699
		Minibatch accuracy: 97.7%
		Validation accuracy: 90.2%
		Test accuracy: 95.6%


	6: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.8 0.9 regularization=0(感觉dropout 很实用效果很好。越深层次的数据越重要，使dropout的值递增会有好处，仅仅猜想)
		Minibatch loss at step 50000 : 0.289883
		Minibatch accuracy: 93.8%
		Validation accuracy: 91.8%
		Test accuracy: 96.2%
		Minibatch loss at step 60000 : 0.175173
		Minibatch accuracy: 93.8%
		Validation accuracy: 91.8%
		Test accuracy: 96.3%
		Minibatch loss at step 70000 : 0.151124
		Minibatch accuracy: 95.3%
		Validation accuracy: 91.7%
		Test accuracy: 96.4%
		Minibatch loss at step 80000 : 0.0946845
		Minibatch accuracy: 96.1%
		Validation accuracy: 91.9%
		Test accuracy: 96.5%

	7: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.5 0.5 
		Minibatch loss at step 80000 : 0.181334
		Minibatch accuracy: 93.8%
		Validation accuracy: 90.7%
		Test accuracy: 95.9%

	8: batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.8 0.9 regularization=0.0001（这样看来regularization 和dropout只取一样就好了。而且dropout会有更好的表现，我觉得接下来可以试试调整学习速率来测试）
		Minibatch loss at step 80000 : 0.342946
		Minibatch accuracy: 93.8%
		Validation accuracy: 90.9%
		Test accuracy: 95.7%

	9：改变网络结构1024，300,50。 用dropout0.5 0.8 0.9进行测试 batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.8 0.9 regularization=0.0
		Minibatch loss at step 80000 : 0.131559
		Minibatch accuracy: 93.8%
		Validation accuracy: 91.5%
		Test accuracy: 96.4%

	10: 改变网络结构1024，300,50。 用dropout0.5 0.8 0.9进行测试 batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.8 0.9 regularization=0.0
		learning_rate = tf.train.exponential_decay(0.5, global_step, 10000, 0.96)
		Minibatch loss at step 79000 : 0.248653
		Minibatch accuracy: 93.8%
		Validation accuracy: 91.9%
		Test accuracy: 96.4%
		Minibatch loss at step 80000 : 0.140172
		Minibatch accuracy: 94.5%
		Validation accuracy: 91.9%
		Test accuracy: 96.3%

	11: 改变网络结构1024，512, 256。 用dropout0.5 0.8 0.9进行测试 batch_size = 128 train_subset = 200000 num_steps = 80001 dropout=0.5 0.8 0.9 regularization=0.0
		learning_rate = tf.train.exponential_decay(0.5, global_step, 10000, 0.96)
			Minibatch loss at step 80000 : 0.104737
			Minibatch accuracy: 95.3%
			Validation accuracy: 92.0%
			Test accuracy: 96.6%
